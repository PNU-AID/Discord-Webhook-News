import os  # ìš´ì˜ì²´ì œì™€ì˜ ìƒí˜¸ì‘ìš©ì„ ë•ëŠ” ë‹¤ì–‘í•œ ê¸°ëŠ¥ ì œê³µ

import requests  # http, https ì›¹ ì‚¬ì´íŠ¸ì— ìš”ì²­í•˜ê¸° ìœ„í•´ ìì£¼ ì‚¬ìš©ë˜ëŠ” ëª¨ë“ˆ
from bs4 import (
    BeautifulSoup,  # ì›¹ ë°ì´í„° í¬ë¡¤ë§ ë„ëŠ” ìŠ¤í¬ë˜í•‘ì„ í•  ë•Œ ì‚¬ìš©í•˜ëŠ” python ë¼ì´ë¸ŒëŸ¬ë¦¬
)
from dotenv import load_dotenv  # .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¡œë“œí•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬

load_dotenv(dotenv_path=".env")  # í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ì— ìˆëŠ” .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¡œë“œ

DISCORD_URL = os.getenv("DISCORD_URL")  # os.getenv()ëŠ” í•¨ìˆ˜ëŠ” ì§€ì •ëœ í™˜ê²½ ë³€ìˆ˜ì˜ ê°’ì„ ë°˜í™˜í•˜ê±°ë‚˜, í•´ë‹¹ í™˜ê²½ ë³€ìˆ˜ê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ Noneì„ ë°˜í™˜
GEEKNEWS_BASEURL = "https://news.hada.io/"
GEEKNEWS_URL = "https://news.hada.io/new"
want_to_include = ["AI", "ai", "ML", "GPT", "LLM", "Diffusion", "ì¸ê³µì§€ëŠ¥", "ë”¥ëŸ¬ë‹", "ë¨¸ì‹ ëŸ¬ë‹", "ì‚¬ì „í•™ìŠµ", "íŒŒì¸íŠœë‹"] # AI ê´€ë ¨ ë‰´ìŠ¤ì¸ì§€ íŒë‹¨í•˜ê¸° ìœ„í•œ ì§€í‘œ
days_included = [f"{i}ì‹œê°„ì „" for i in range(1, 23)] # 24ì‹œê°„ ë‚´ ë‰´ìŠ¤ì¸ì§€ í™•ì¸

headers = {"Content-Type": "application/json"}
messages = []

response = requests.get(GEEKNEWS_URL)
if response.status_code == 200:
    html = response.text
    soup = BeautifulSoup(html, "html.parser")
    topics = soup.find_all("div", class_="topic_row")
    
    for topic in topics:
        # ë‰´ìŠ¤ ë‚´ tag ì§€ì •
        topic_title = topic.find("div", class_="topictitle") # ë‰´ìŠ¤ ì œëª©
        topic_content = topic.find("div", class_="topicdesc") # ë‰´ìŠ¤ ë‚´ìš©
        topic_url = topic.find("span", class_="topicurl") # ë‰´ìŠ¤ ë§í¬
        topic_info = topic.find("div", class_="topicinfo") # ë‰´ìŠ¤ ì •ë³´
        
        # ë‰´ìŠ¤ ì œëª© ì¶”ì¶œ 
        if topic_title.a is None:
            title = topic_title.h1.string
        else:
            title = topic_title.a.h1.string
            # topic_link = topic_title.find("a")["href"].strip()
        
        # ë‰´ìŠ¤ ë‚´ìš© ì¶”ì¶œ
        description = ""
        if topic_content is not None:
            description = topic_content.a.string
            topic_link = topic_content.find("a")["href"].strip()
        else:
            topic_link = topic_title.find("a")["href"].strip()

        # ë‰´ìŠ¤ urlì´ ì—†ìœ¼ë©´ ê±´ë„ˆë€œ
        if topic_url.string is None:
            continue

        # ë‰´ìŠ¤ ë‚ ì§œ ì¶”ì¶œí•´ì„œ 24ì‹œê°„ ë‚´ ë‚´ìš©ì¸ì§€ í™•ì¸
        date_flag = False
        topic_date = topic_info.text.split(" ")[4]
        for days in days_included:
            if days == topic_date:
                date_flag = True
                break
            
        # 24ì‹œê°„ì´ ì§€ë‚œ ë‰´ìŠ¤ëŠ” ê±´ë„ˆëœ€
        if not date_flag:
            continue

        # AI ê´€ë ¨ ë‰´ìŠ¤ì¸ì§€ í™•ì¸
        for word in want_to_include:
            if word in title or word in description:
                message = f"## ğŸ“{title}\n\n ë§í¬ : {GEEKNEWS_BASEURL + topic_link}"
                messages.append(message)
                break
else:
    print(f"Error: {response.status_code}")

# ë””ìŠ¤ì½”ë“œ ì „ì†¡
if len(messages) != 0:
    for message in messages:
        data = {"content": message}
        response = requests.post(DISCORD_URL, json=data)
        # print(message)
        if response.status_code == 404:
            print("response code is 404, check discord url or action variable")
